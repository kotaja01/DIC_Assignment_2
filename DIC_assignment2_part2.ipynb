{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da74ea3f-f964-466a-b9a2-ed44cacfd80a",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c658d6b9-84f5-4be8-8cf5-5c7a451a0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, split, col, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF, ChiSqSelector, StringIndexer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7576e-1296-4e57-8ffc-cda0bd0c0f3a",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1270fc-7fcd-46a9-b6e7-66fb9308a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_file_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "\n",
    "output_file_path = \"output_assignment2.txt\"\n",
    "\n",
    "stop_words_path = \"stopwords.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbf22d-3a99-48fe-ada6-e29d58a35fa8",
   "metadata": {},
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6b454c-6015-4bb5-965c-ca36132f3e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/30 17:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/30 17:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/30 17:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/04/30 17:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/04/30 17:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/04/30 17:30:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Assignment_2_session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb8ba4-fa3a-449f-ba90-6d76bf60ea02",
   "metadata": {},
   "source": [
    "# Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4dd4197-81db-4a9e-9355-37e1a60b1533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_full = spark.read.json(dev_file_path)\n",
    "#df_full = spark.read.json(full_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c05e6-9ddb-4299-8b10-dccb4156befb",
   "metadata": {},
   "source": [
    "# Select columns category and reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7705d47-9cd8-4478-9e6a-ec9f24988bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.select(\"reviewText\", \"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355dd747-2f8f-4b1d-931d-b2fb449dbbe0",
   "metadata": {},
   "source": [
    "# Create custom casefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f55b18-3bf1-4940-924b-6c0f0e39fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define udf\n",
    "@udf(returnType=StringType())\n",
    "def casefold_text(text):\n",
    "    if text is not None:\n",
    "        return text.casefold()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a custom casefolder\n",
    "class CasefoldTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCol=\"text\", outputCol=\"casefolded_text\"):\n",
    "        super(CasefoldTransformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, df):\n",
    "        return df.withColumn(self.outputCol, casefold_text(df[self.inputCol]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9a2a4-a4a7-4821-b41a-d0b9a5a628d5",
   "metadata": {},
   "source": [
    "# Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5125141-bda6-40b0-9b23-c1e609664858",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stop_words_path, \"r\") as stop_words_file:\n",
    "    stop_words = list(stop_words_file.read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1a7e9-151e-474b-a62b-58ee488432e8",
   "metadata": {},
   "source": [
    "# Define steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afcd0b-a10e-409c-b845-ae1990b25727",
   "metadata": {},
   "source": [
    "## Casefolding -> Tokenization -> Stopwords removal -> TF-IDF calculation -> Chi square selection (using 2000 top terms overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fa67ba-dbc4-4e7e-9b54-c15486340c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "casefolder = CasefoldTransformer(inputCol=\"reviewText\", outputCol=\"reviewText_casefolded\")\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText_casefolded\", outputCol=\"tokens\", \n",
    "                           pattern=\"[ \\t\\d\\(\\)\\[\\]\\{\\}\\.\\!\\?,;:+=\\-_'\\\"`~#@&\\*\\%€\\$§\\\\/]+\", gaps=True, toLowercase=False)\n",
    "\n",
    "remover = StopWordsRemover(inputCol = \"tokens\", outputCol = \"filtered_tokens\", stopWords = stop_words)\n",
    "\n",
    "tf = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"category_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95098eae-ba69-426e-9151-ec9556bc7956",
   "metadata": {},
   "source": [
    "# Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03dea973-10b2-4849-ace7-8c3cc868c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [casefolder, tokenizer, remover, tf, idf, indexer, selector])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339f1b0-4b54-4c9d-b761-81a846261a0e",
   "metadata": {},
   "source": [
    "# Apply pipeline on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3432110f-e832-4ce3-b503-2d0460257f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/30 17:31:27 WARN DAGScheduler: Broadcasting large task binary with size 1066.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/30 17:31:41 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/30 17:31:41 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/30 17:31:50 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c3d17-5fe6-45cf-8ed4-4ba2ebcfef95",
   "metadata": {},
   "source": [
    "# Extract vocabulary, selected features and save selected terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6192e72-bce0-4c31-832d-bbf97f754dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = model.stages[3].vocabulary\n",
    "\n",
    "selected_features = model.stages[6].selectedFeatures\n",
    "\n",
    "selected_terms = sorted([vocabulary[i] for i in selected_features])\n",
    "\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    f.write(\" \".join(selected_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d5754-93dc-4d96-af4b-f05843abb627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
